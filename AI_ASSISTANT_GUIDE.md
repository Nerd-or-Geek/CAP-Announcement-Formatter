# AI-Powered Formatting Assistant - Complete Guide

## Overview

The CAP Announcement Formatter now includes **free AI support** using **Ollama** - a local LLM (Large Language Model) that runs on your computer without requiring internet or cloud services.

**Key Features**:
- âœ… Free and open-source (no API costs)
- âœ… Runs locally (no data sent to cloud)
- âœ… Offline operation (works without internet after setup)
- âœ… Smart model selection (based on your system)
- âœ… Three quality levels: Good/Fast, Better/Medium, Best/Slow

---

## Quick Start

### Step 1: Click "Setup AI" Button
In the application, click the **ðŸ¤– Setup AI** button in the top toolbar.

### Step 2: Check System
The setup dialog will:
- Detect your system RAM
- Recommend appropriate model (Good/Better/Best)
- Check if Ollama is installed

### Step 3: Choose Quality Level
Select one of three options:

| Level | Model | Download | Speed | RAM Required |
|-------|-------|----------|-------|--------------|
| Good/Fast | 3B | ~2GB | 2-3 sec | 4GB |
| Better/Medium | 8B | ~4.7GB | 5-8 sec | 8GB |
| Best/Slow | 70B | ~39GB | 20-30 sec | 64GB |

### Step 4: Download & Setup
- If Ollama is not installed, click "Download Ollama" button
- Install Ollama (runs in background)
- Return to app and select your model
- Wait for download to complete

### Step 5: Start Improving
Once setup is complete:
- Select a widget in the document
- Click âœ¨ **Improve with AI** button
- AI will enhance the content

---

## How It Works

### System Detection

The app automatically detects:
- **Total RAM** - Used to recommend appropriate model
- **Ollama Installation** - Checks if already installed
- **Downloaded Models** - Lists available models

**Detection Process**:
```
Windows: wmic ComputerSystem get TotalPhysicalMemory
Linux: /proc/meminfo MemTotal
macOS: sysctl -n hw.memsize
```

### Improvement Process

When you click "Improve with AI":

```
1. App detects selected widget
2. Extracts content from multiline field
3. Sends to Ollama with prompt:
   "Improve this announcement by fixing grammar,
    improving clarity, maintaining professional tone"
4. Ollama processes and returns improved text
5. Content is updated in real-time
6. Document marked as unsaved
```

### AI Models

**Ollama Models Used** (all LLaMA 3.2 variants):

| Model | Parameters | Speed | Quality | RAM |
|-------|-----------|-------|---------|-----|
| `llama3.2:3b` | 3 Billion | âš¡âš¡âš¡ Fast | Good | 4GB |
| `llama3.2:8b` | 8 Billion | âš¡âš¡ Medium | Better | 8GB |
| `llama3.2:70b` | 70 Billion | âš¡ Slow | Best | 64GB |

**Downloads**:
- 3B: ~2GB (15 minutes on good internet)
- 8B: ~4.7GB (45 minutes)
- 70B: ~39GB (several hours)

---

## Features & Commands

### AI Setup
**Button**: ðŸ¤– Setup AI  
**When**: First time using AI  
**Does**: Opens setup wizard, installs Ollama if needed, downloads AI model

### Improve with AI
**Button**: âœ¨ Improve with AI  
**When**: After setup, with widget selected  
**Does**: Enhances selected widget's content

**Improvements Made**:
- âœ… Grammar & spelling fixes
- âœ… Clarity improvements
- âœ… Professional tone enhancement
- âœ… Readability optimization
- âœ… Preserves HTML structure
- âœ… Keeps template variables intact

### Get Suggestions
**Function**: Get AI Suggestions  
**Purpose**: Ask AI for improvement ideas without modifying content  
**Output**: Numbered list of suggestions

---

## System Requirements

### Minimum Requirements
- **RAM**: 4GB (for Good/3B model)
- **Disk**: 5GB free space
- **OS**: Windows, Mac, or Linux
- **Internet**: For initial download only

### Recommended for Each Model

**Good/Fast (3B Model)**
```
CPU: Any modern processor
RAM: 4GB minimum, 8GB+ recommended
Disk: 3GB free
GPU: Not required (uses CPU)
```

**Better/Medium (8B Model)**
```
CPU: Multi-core processor
RAM: 8GB minimum, 16GB+ recommended
Disk: 6GB free
GPU: Optional (NVIDIA/AMD for speed)
```

**Best/Slow (70B Model)**
```
CPU: High-end multi-core
RAM: 64GB+ required
Disk: 50GB free
GPU: Highly recommended (A100, H100, RTX 3090)
```

---

## Installation Details

### What Ollama Does

Ollama is a lightweight container system that:
1. Manages AI models locally
2. Provides HTTP API (http://localhost:11434)
3. Runs models in isolated environment
4. Uses your system resources (CPU/GPU)

### Ollama Installation

**Windows**:
1. Download from https://ollama.com/download
2. Run OllamaSetup.exe
3. Follow installer prompts
4. Ollama starts automatically

**macOS**:
1. Download from https://ollama.com/download
2. Unzip to Applications
3. Launch Ollama.app
4. Service runs in background

**Linux**:
```bash
curl https://ollama.ai/install.sh | sh
ollama serve  # Run in terminal
```

### Starting Ollama

**Windows/macOS**: Runs automatically after installation

**Linux**: Start manually:
```bash
ollama serve
```

Ollama listens on: `http://localhost:11434`

---

## AI API Implementation

### OllamaService Class

**Location**: `AnnouncementFormatter.Core/Services/OllamaService.cs`

**Key Methods**:

```csharp
// Check if Ollama is installed
public async Task<bool> IsOllamaInstalledAsync()

// Get list of downloaded models
public async Task<List<OllamaModel>> GetInstalledModelsAsync()

// Download a model
public async Task<bool> PullModelAsync(string modelName, IProgress<string> progress)

// Improve text content
public async Task<string> ImproveFormattingAsync(string content, string context)

// Get improvement suggestions
public async Task<string> GetSuggestionsAsync(string content)

// Set active model
public void SetModel(string modelName)

// Get recommended model for system
public static string GetRecommendedModel(SystemCapability capability)
```

### SystemDetector Class

**Location**: `AnnouncementFormatter.Core/Services/SystemDetector.cs`

**Key Methods**:

```csharp
// Detect system capability level
public static SystemCapability DetectCapability()

// Get total RAM in GB
public static double GetTotalRamGB()

// Get system information summary
public static string GetSystemSummary()

// Get user-friendly capability description
public static string GetCapabilityDescription(SystemCapability capability)

// Check if Ollama executable is installed
public static bool IsOllamaExecutableInstalled()

// Get download URL for Ollama
public static string GetOllamaDownloadUrl()
```

### API Endpoints Used

**Check Version**:
```
GET /api/version
Returns: { "version": "0.1.0" }
```

**List Models**:
```
GET /api/tags
Returns: { "models": [{ "name": "llama3.2:3b", "size": 2000000000 }] }
```

**Pull/Download Model**:
```
POST /api/pull
{
  "name": "llama3.2:3b",
  "stream": true
}
Streams: { "status": "Downloading..." }
```

**Generate Text**:
```
POST /api/generate
{
  "model": "llama3.2:3b",
  "prompt": "Your prompt here",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.9
  }
}
Returns: { "response": "Generated text..." }
```

---

## Error Handling

### Common Issues & Solutions

**Issue**: "Ollama Not Installed"
- **Cause**: Ollama not found in system PATH
- **Solution**: Download and install Ollama from ollama.com

**Issue**: "Connection Refused"
- **Cause**: Ollama service not running
- **Solution**: Start Ollama service manually

**Issue**: "Out of Memory"
- **Cause**: Model too large for your RAM
- **Solution**: Choose smaller model (Good instead of Best)

**Issue**: "Model Download Fails"
- **Cause**: Network issue or storage full
- **Solution**: Check internet, clear disk space, try again

**Issue**: "AI Feature Greyed Out"
- **Cause**: No model downloaded yet
- **Solution**: Run setup and let model download complete

---

## Performance Tips

### For Good/Fast Performance (3B Model)

1. **Close other applications** - Free up RAM
2. **First run is slower** - Model needs to load into memory
3. **Subsequent requests faster** - Model stays in memory
4. **2-3 second response time** - Normal and expected

### For Better Performance (8B Model)

1. **Use 16GB+ RAM system** - Avoid slowness
2. **Dedicated GPU helps** - NVIDIA CUDA accelerates processing
3. **5-8 second response time** - Expected
4. **Don't run other heavy apps** - Improve response time

### For Best Performance (70B Model)

1. **64GB+ RAM required** - CPU only is very slow
2. **High-end GPU essential** - Need A100, H100, or RTX 3090+
3. **20-30 second response time** - Expected
4. **Not practical for most users** - Only for servers/workstations

### Optimization Settings

**OllamaService Configuration**:
```csharp
// Temperature: 0-1
// Lower = more predictable/consistent
temperature = 0.3

// Top-P: 0-1  
// Lower = more focused
top_p = 0.9
```

These are tuned for announcement formatting.

---

## Advanced Usage

### Using Different Models

You can manually specify any Ollama model:

```csharp
var service = new OllamaService();
service.SetModel("mistral:7b");  // Use Mistral instead
var improved = await service.ImproveFormattingAsync(content);
```

**Available Models on Ollama**:
- llama3.2 (3b, 8b, 70b) - Recommended
- mistral (7b)
- neural-chat (7b)
- dolphin-mixtral (8x7b)
- And many more...

### Custom Prompts

Edit prompts in `OllamaService.cs`:

```csharp
var prompt = $@"You are an expert in formatting announcements.
Please improve the following content...";
```

Change the instructions to customize AI behavior.

### Batch Processing

Process multiple widgets:

```csharp
foreach (var widget in DocumentWidgets)
{
    var improved = await service.ImproveFormattingAsync(
        widget.GetFieldValue("content")
    );
    widget.SetFieldValue("content", improved);
}
```

---

## Privacy & Security

### Data Handling

âœ… **All processing is local**:
- No data sent to cloud servers
- No API keys required
- No tracking or analytics
- No personal data collection

âœ… **Secure by default**:
- Ollama runs on localhost only
- No network exposure
- Encrypted communication with localhost
- Complete user control

### Model Safety

- **LLaMA 3.2** - Trained with safety guidelines
- **Content Filtering** - Inappropriate content filtered
- **No proprietary training data** - Built on public datasets
- **Transparent model** - Open source, auditable

---

## Troubleshooting

### Setup Issues

**Ollama not detected**
1. Open command prompt
2. Type: `ollama --version`
3. If not found, download from ollama.com
4. Restart application after installation

**Model download fails**
1. Check internet connection
2. Verify you have 5GB+ free disk space
3. Check firewall (port 5500 for downloads)
4. Try again later

**Out of memory during download**
1. Close other applications
2. Choose smaller model (3B instead of 8B)
3. Free up RAM

### Runtime Issues

**"AI Ready" but button greyed out**
1. Make sure widget is selected (highlighted in canvas)
2. Widget must have content to improve
3. Refresh by clicking another widget first

**Improvement takes too long**
1. Normal for first request (model loading)
2. Subsequent requests are faster
3. Close other apps to speed up
4. Check CPU/RAM usage

**Improved text looks wrong**
1. This is rare with LLaMA 3.2
2. Try again (random variation is normal)
3. Try "Get Suggestions" instead
4. Manually edit if needed

---

## Files & Architecture

### New Files Created

```
src/AnnouncementFormatter.Core/Services/
â”œâ”€â”€ OllamaService.cs           # Main Ollama API wrapper
â””â”€â”€ SystemDetector.cs          # System capability detection

src/AnnouncementFormatter/Views/
â”œâ”€â”€ OllamaSetupWindow.axaml    # Setup UI
â””â”€â”€ OllamaSetupWindow.axaml.cs # Setup code-behind

src/AnnouncementFormatter/ViewModels/
â””â”€â”€ OllamaSetupViewModel.cs    # Setup logic
```

### Modified Files

```
src/AnnouncementFormatter/ViewModels/
â”œâ”€â”€ MainWindowViewModel.cs     # Added AI commands
â””â”€â”€ App.axaml.cs              # Added MainWindow reference

src/AnnouncementFormatter/Views/
â””â”€â”€ MainWindow.axaml          # Added AI buttons
```

### Architecture Diagram

```
MainWindow UI
    â†“
MainWindowViewModel (SetupAI, ImproveWithAI commands)
    â†“
OllamaService (HTTP to Ollama)
    â†“
Ollama Service (localhost:11434)
    â†“
Local LLM Model (3B/8B/70B)
    â†“
Improved Text Output
```

---

## Future Enhancements

### Planned Features

1. **Batch Processing**
   - Improve all widgets at once
   - Queue system for large documents

2. **Suggestion Panel**
   - Show AI suggestions in separate panel
   - Pick and choose improvements

3. **Custom Prompts**
   - User-defined improvement instructions
   - Save favorite prompts

4. **Model Management**
   - Switch models in-app
   - Manage downloaded models
   - Auto-unload unused models

5. **Quality Metrics**
   - Show improvement score
   - Before/after comparison
   - Suggestion confidence level

---

## Comparison: AI Solutions

| Feature | Our Solution | Cloud AI | Other Local |
|---------|-------------|----------|------------|
| **Cost** | Free | $$$/month | Free |
| **Privacy** | 100% Local | Cloud stored | Depends |
| **Speed** | 2-30 sec | 1-5 sec | 2-30 sec |
| **Requires Internet** | No (after setup) | Yes | No |
| **Model Selection** | 3 options | 1-2 | Many |
| **Setup Difficulty** | Easy | Very Easy | Medium |
| **Customization** | High | Low | High |
| **Data Storage** | Local only | Cloud | Local only |

---

## Resources

### Official Links
- **Ollama**: https://ollama.com
- **LLaMA Model**: https://llama.meta.com
- **Ollama GitHub**: https://github.com/jmorganca/ollama

### Documentation
- **Ollama API Docs**: https://github.com/jmorganca/ollama/blob/main/docs/api.md
- **Model Library**: https://ollama.com/library

### System Monitoring
- **Windows**: Task Manager (Processes tab)
- **Mac**: Activity Monitor
- **Linux**: `top` or `htop`

---

## Summary

The AI-powered formatting assistant:
- âœ… Detects your system automatically
- âœ… Recommends appropriate model
- âœ… Downloads and installs locally
- âœ… Improves announcement formatting
- âœ… Maintains privacy (no cloud)
- âœ… Works offline (after setup)
- âœ… Completely free

**To get started**: Click ðŸ¤– **Setup AI** in the toolbar!
